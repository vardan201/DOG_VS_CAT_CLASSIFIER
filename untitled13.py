# -*- coding: utf-8 -*-
"""Untitled13.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XcH6N-T_sjDrvRVjwlU-9LcTr22iqxVv
"""
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets download -d salader/dogs-vs-cats



import zipfile
zip_ref = zipfile.ZipFile('/content/dogs-vs-cats.zip', 'r')
zip_ref.extractall('/content')
zip_ref.close()

#DATA AUGMENTATION
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications.resnet50 import preprocess_input

train_datagen = ImageDataGenerator(
    preprocessing_function=preprocess_input,
    rotation_range=20,
    width_shift_range=0.1,
    height_shift_range=0.1,
    shear_range=0.1,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

# Validation data (no augmentation, just preprocessing)
val_datagen = ImageDataGenerator(
    preprocessing_function=preprocess_input
)

train_generator = train_datagen.flow_from_directory(
    directory='/content/train',


    target_size=(224, 224),
    batch_size=32,
    class_mode='binary'
)

val_generator = val_datagen.flow_from_directory(
    directory='/content/test',
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary'
)

pip install -q optuna

import optuna
from tensorflow.keras.applications import ResNet50
from tensorflow.keras import layers, models, optimizers, callbacks, regularizers
from tensorflow.keras.backend import clear_session

def objective(trial):
    clear_session()  # Clean slate for each trial

    # Hyperparameters to tune
    dense_units = trial.suggest_categorical('dense_units', [128, 256, 512])
    dropout_rate = trial.suggest_float('dropout_rate', 0.3, 0.6)
    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)
    unfreeze_layers = trial.suggest_int('unfreeze_layers', 10, 50, step=10)
    l2_reg = trial.suggest_float('l2_reg', 1e-5, 1e-3, log=True)

    # Load pretrained base model
    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
    base_model.trainable = True

    # Freeze all but last `unfreeze_layers`
    for layer in base_model.layers[:-unfreeze_layers]:
        layer.trainable = False

    # Build model with BatchNorm & L2 regularization
    model = models.Sequential([
        base_model,
        layers.GlobalAveragePooling2D(),

        layers.Dense(
            dense_units,
            kernel_regularizer=regularizers.l2(l2_reg)
        ),
        layers.BatchNormalization(),
        layers.Activation('relu'),
        layers.Dropout(dropout_rate),

        layers.Dense(
            1,
            activation='sigmoid',
            kernel_regularizer=regularizers.l2(l2_reg)
        )
    ])

    # Compile
    model.compile(
        optimizer=optimizers.Adam(learning_rate=learning_rate),
        loss='binary_crossentropy',
        metrics=['accuracy']
    )

    # Early stopping callback
    es = callbacks.EarlyStopping(monitor='val_accuracy', patience=2, restore_best_weights=True)

    # Train
    history = model.fit(
        train_generator,
        validation_data=val_generator,
        epochs=10,
        callbacks=[es],
        verbose=1
    )

    # Return best validation accuracy
    val_acc = max(history.history['val_accuracy'])
    return val_acc

study = optuna.create_study(direction='maximize')  # maximize val_accuracy
study.optimize(objective, n_trials=10)  # run 10 trials (you can increase)

print("Best trial:")
print(f"  Value: {study.best_value}")  # best val_accuracy
print("  Params: ")
for key, value in study.best_params.items():
    print(f"    {key}: {value}")

best_params = study.best_params

# For example, you can create a function that builds the model with params:
def build_model(params):
    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224,224,3))
    base_model.trainable = True
    for layer in base_model.layers[:-params['unfreeze_layers']]:
        layer.trainable = False

    model = models.Sequential([
        base_model,
        layers.GlobalAveragePooling2D(),
        layers.Dense(params['dense_units'], kernel_regularizer=regularizers.l2(params['l2_reg'])),
        layers.BatchNormalization(),
        layers.Activation('relu'),
        layers.Dropout(params['dropout_rate']),
        layers.Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(params['l2_reg']))
    ])

    model.compile(optimizer=optimizers.Adam(learning_rate=params['learning_rate']),
                  loss='binary_crossentropy', metrics=['accuracy'])
    return model

final_model = build_model(best_params)

# Train final model (more epochs, or on train+val)
final_model.fit(train_generator, validation_data=val_generator, epochs=10)

import keras

keras.saving.save_model(final_model, 'my_model.keras')

